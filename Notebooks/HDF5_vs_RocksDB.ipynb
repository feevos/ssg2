{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d5058ed-68c2-4ffc-a691-918c14dbfd6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%pylab is deprecated, use %matplotlib inline and import the required libraries.\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d926082-0c61-4a17-83a9-d2fa6c155fad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dia021/.local/lib/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0f1a914-83b3-4ae4-8020-ffdfe8906c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "\n",
    "mnist_dataset = torchvision.datasets.MNIST(root='./mnist/', train=True,download=True, transform=transform_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc1d2e3-b97b-4ad3-9230-421cc4afa379",
   "metadata": {},
   "source": [
    "# Writing benchmark "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1cfd99c2-b633-4b55-a12a-427c46eac0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60ab5bb7-bb54-4758-b02f-bd11c4845754",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "class HDF5DatasetWriter(object):\n",
    "    def __init__(self, filename, dataset):\n",
    "        super().__init__()\n",
    "\n",
    "\n",
    "        # This is a raster type dataset \n",
    "        self.dataset = dataset\n",
    "        # Find dtype, shape and length of the array\n",
    "        n = len(self.dataset)\n",
    "        inputs, labels = self.dataset[0]\n",
    "        inputs = inputs.numpy()\n",
    "        if isinstance(inputs,list):\n",
    "            c,h,w = inputs[0].shape\n",
    "            inputs_dtype = inputs[0].dtype\n",
    "        else:\n",
    "            c,h,w = inputs.shape\n",
    "            inputs_dtype = inputs.dtype\n",
    "\n",
    "        #cm = labels.shape[0]\n",
    "        labels_dtype = np.int64 # labels.dtype\n",
    "\n",
    "        self.hdf5 = h5py.File(filename,\"a\")\n",
    "        # Tailored for MNIST\n",
    "        self.hdf5.create_dataset('inputs',(n,c,h,w),dtype=inputs_dtype)\n",
    "        self.hdf5.create_dataset('labels',(n,),dtype=labels_dtype)\n",
    "\n",
    "    def write_element(self,i):\n",
    "\n",
    "        inputs, labels = self.dataset[i]\n",
    "        self.hdf5['inputs'][i] = inputs\n",
    "        self.hdf5['labels'][i] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58ba34ce-d6e1-4cd3-aec7-8d4a0a0d1b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "myHDF5MNISTWriter = HDF5DatasetWriter(filename=\"mnist.hdf5\",dataset=mnist_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e922170-1ca0-4e3d-a6f4-7453e0415ad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time per datum:0.0003399996558825175sec\n",
      "Datums per Second::2941.1794473860796 - SERIAL\n",
      "time to WRITE 60000 data::20.39997935295105sec\n"
     ]
    }
   ],
   "source": [
    "tic = time()\n",
    "for i in range(len(mnist_dataset)):\n",
    "    myHDF5MNISTWriter.write_element(i)\n",
    "Dt = time() - tic\n",
    "NData = len(mnist_dataset)\n",
    "print(\"Time per datum:{}sec\".format(Dt/NData))\n",
    "print(\"Datums per Second::{} - SERIAL\".format(NData/Dt))\n",
    "print(\"time to WRITE {} data::{}sec\".format(NData,Dt))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "43aae929-859d-44af-90fa-96c58e4cf1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "myHDF5MNISTWriter.hdf5.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3b321ac4-2129-43ed-b910-9de5ff7a2936",
   "metadata": {},
   "outputs": [],
   "source": [
    "myHDF5MNISTWriter = HDF5DatasetWriterChunks(filename=\"mnist_chunked.hdf5\",dataset=mnist_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "be0d73fb-6c9f-4eeb-b83a-5c860bdceafa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time per datum:0.0003361504395802816sec\n",
      "Datums per Second::2974.858522418126 - SERIAL\n",
      "time to WRITE 60000 data::20.169026374816895sec\n"
     ]
    }
   ],
   "source": [
    "tic = time()\n",
    "for i in range(len(mnist_dataset)):\n",
    "    myHDF5MNISTWriter.write_element(i)\n",
    "Dt = time() - tic\n",
    "NData = len(mnist_dataset)\n",
    "print(\"Time per datum:{}sec\".format(Dt/NData))\n",
    "print(\"Datums per Second::{} - SERIAL\".format(NData/Dt))\n",
    "print(\"time to WRITE {} data::{}sec\".format(NData,Dt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956dd7c1-40bc-4247-9b46-863ba2cc9d8a",
   "metadata": {},
   "source": [
    "# Now let's see RocksDB "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48f80bba-325e-4fda-bc9b-be2a0cd956d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rocksdb\n",
    "import pickle\n",
    "class RocksDBDatasetWriter(object):\n",
    "\n",
    "    def __init__(self, flname_db, metadata, dataset):\n",
    "        self.db = None\n",
    "        #self.lock = Lock()\n",
    "        self.dataset=dataset\n",
    "\n",
    "\n",
    "        self.flname_db = flname_db\n",
    "        self.meta = metadata\n",
    "\n",
    "    def _open_rocks(self):\n",
    "\n",
    "\n",
    "        # ==========================================================================\n",
    "        # TODO : they need further investigation \n",
    "        # Some good behaving defaults \n",
    "        opts = rocksdb.Options()\n",
    "        opts.create_if_missing = True\n",
    "        opts.max_open_files = 300000\n",
    "        opts.write_buffer_size = 67108864\n",
    "        opts.max_write_buffer_number = 30 # 3 default\n",
    "        opts.target_file_size_base = 67108864  # default 67108864, value starting 7: 7340032 input.nbytes\n",
    "        opts.paranoid_checks=False\n",
    "\n",
    "        # @@@@@@@@@@@ NEW LINE @@@@@@@@@@@@@@@@@@@\n",
    "        opts.IncreaseParallelism()\n",
    "        # @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
    "        \n",
    "        opts.table_factory = rocksdb.BlockBasedTableFactory(\n",
    "                filter_policy=rocksdb.BloomFilterPolicy(10), # 10 default\n",
    "                block_cache=rocksdb.LRUCache( 2 * (1024 ** 3)), # 2 * (1024 ** 3) default\n",
    "                block_cache_compressed=rocksdb.LRUCache(500 * (1024 ** 2)))\n",
    "        # ==================================================================================\n",
    "\n",
    "        self.db = rocksdb.DB(self.flname_db, opts,read_only=False)\n",
    "\n",
    "        # REMARK: it is possible to create \"column_family\", so as to have two distince separate categories of variables (actually as many \n",
    "        # dimensions as desired). This MUst improve search capabilities\n",
    "        # See \n",
    "        \n",
    "        \n",
    "        meta_key = 'meta'.encode('ascii')\n",
    "        meta_dumps = pickle.dumps(self.meta)\n",
    "        self.db.put(meta_key,meta_dumps)\n",
    "\n",
    "    def write_element(self,idx):\n",
    "        if self.db is None:\n",
    "            self._open_rocks()\n",
    "\n",
    "        # Tailored for mnist \n",
    "        imgs, labels = self.dataset[idx]\n",
    "        imgs = imgs.numpy().tobytes()\n",
    "        labels = np.array([labels]).astype(np.uint8).tobytes()\n",
    "        \n",
    "        if isinstance(imgs,list):\n",
    "            n_imgs = len(imgs)\n",
    "            for i, img in enumerate(imgs):\n",
    "                key_img = 'inputs{}_{}'.format(i,idx).encode('ascii')\n",
    "                img = img.tobytes()\n",
    "                self.db.put(key_img,  img)\n",
    "        else:\n",
    "            key_img  = 'inputs_{}'.format(idx).encode('ascii')\n",
    "            self.db.put(key_img,imgs)\n",
    "\n",
    "        key_mask = 'labels_{}'.format(idx).encode('ascii')\n",
    "        self.db.put(key_mask, labels)\n",
    "\n",
    "    # Optionally one can write in batches, to increase parallelism. \n",
    "    def write_batch(self, batch_idx, databatch):\n",
    "        if self.db is None:\n",
    "            self._open_rocks()\n",
    "\n",
    "        inputs, labels = zip(*databatch)\n",
    "        inputs = np.array(inputs)\n",
    "        labels = np.array(inputs)\n",
    "        batch_size = inputs.shape[0]\n",
    "        bb =  rocksdb.WriteBatch()\n",
    "        for idx, (tinput, tlabel) in enumerate(zip(inputs,labels)):\n",
    "            key_input = 'inputs_{}'.format(idx+batch_idx*batch_size).encode('ascii')\n",
    "            bb.put(key_input,tinput.tobytes())\n",
    "            key_input = 'labels_{}'.format(idx+batch_idx*batch_size).encode('ascii')\n",
    "            bb.put(key_input,tlabel.tobytes())\n",
    "        self.db.write(bb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "69b4d709-5f69-4568-aaf6-cd39a0d31a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "myRocksDBWriter = RocksDBDatasetWriter(flname_db=\"mnist_rocks_batch.db\",\n",
    "                                       metadata={'inputs_shape':(1,28,28),\n",
    "                                                'inputs_dtype':np.float32,\n",
    "                                                'labels_shape':None,\n",
    "                                                'labels_dtype':np.uint8},\n",
    "                                      dataset=mnist_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "86f98e37-65d1-4a16-b2db-19b43f2020dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time per datum:0.00010444822311401367sec\n",
      "Datums per Second::9574.121705339297 - SERIAL\n",
      "time to WRITE 60000 data::6.26689338684082sec\n"
     ]
    }
   ],
   "source": [
    "tic = time()\n",
    "for i in range(len(mnist_dataset)):\n",
    "    myRocksDBWriter.write_element(i)\n",
    "Dt = time() - tic\n",
    "NData = len(mnist_dataset)\n",
    "print(\"Time per datum:{}sec\".format(Dt/NData))\n",
    "print(\"Datums per Second::{} - SERIAL\".format(NData/Dt))\n",
    "print(\"time to WRITE {} data::{}sec\".format(NData,Dt))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec012d1-3974-4d8b-ad87-dd9f9137e5f9",
   "metadata": {},
   "source": [
    "## Therefore RocksDB is 20.20 / 6.73 = 3 times faster when writing elementwise \n",
    "\n",
    "\n",
    "How about writing in batches? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a53bcda5-bb5b-4a9a-a33a-bb5bbd99afa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(r'../../')\n",
    "# Here I am using the class provided with this repo\n",
    "from ssg2.data.rocksdbutils import RocksDBWriter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60cf23bd-5ed3-49ab-b168-357285270c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict as odict\n",
    "F=28\n",
    "meta = odict({'inputs'  : odict({'inputs_shape' : (1, F,F),\n",
    "                                    'inputs_dtype' : np.float32}),\n",
    "                 'labels'  : odict({'labels_shape' : None,\n",
    "                                    'labels_dtype' : np.uint8})})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06ea6070-e2d9-4bfb-a019-31b9ef1056eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The num_workers is a hyperparameter, 4 works best for this dataset\n",
    "myDBWriter = RocksDBWriter(flname_db='mnist_RDB_v2.db',metadata=meta,num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b172d6db-e812-45b6-8501-819fac8bdd9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing - make them in batches, this is slow as it is serial, but we can parallelize it easily - see below\n",
    "temp = [ [datum[0].numpy(),np.array(datum[1],dtype=np.uint8)] for datum in mnist_dataset]\n",
    "n=100 # Write 100 datums within batch\n",
    "databatches = [temp[i:i+n] for i in range(0,len(temp),n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e2261e60-b8a6-48b4-9828-bce65d8cc3f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time per datum:2.99858291943868e-05sec\n",
      "Datums per Second::33349.08611388993 - WriteBatch\n",
      "time to WRITE 60000 data::1.799149751663208sec\n"
     ]
    }
   ],
   "source": [
    "# Timing only the write operation \n",
    "tic = time()\n",
    "for datum in databatches:\n",
    "    myDBWriter.write_batch(datum)\n",
    "Dt = time() - tic\n",
    "NData = len(mnist_dataset)\n",
    "print(\"Time per datum:{}sec\".format(Dt/NData))\n",
    "print(\"Datums per Second::{} - WriteBatch\".format(NData/Dt))\n",
    "print(\"time to WRITE {} data::{}sec\".format(NData,Dt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3542bbbe-4aac-4a34-b187-0e624c1d47be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool, cpu_count\n",
    "\n",
    "def prepare_data(start_end):\n",
    "    start, end = start_end\n",
    "    temp = []\n",
    "    for i in range(start, end):\n",
    "        datum = mnist_dataset[i]\n",
    "        temp.append([datum[0].numpy(), np.array(datum[1], dtype=np.uint8)])\n",
    "    return temp\n",
    "\n",
    "# Number of processes to spawn. You can also use fewer processes if you'd like.\n",
    "num_processes = cpu_count()\n",
    "\n",
    "# Calculate indices for splitting the dataset\n",
    "dataset_size = len(mnist_dataset)\n",
    "chunk_size = dataset_size // num_processes\n",
    "indices = [(i, i + chunk_size) for i in range(0, dataset_size, chunk_size)]\n",
    "\n",
    "# Use multiprocessing to prepare data in parallel\n",
    "with Pool(num_processes) as pool:\n",
    "    parallel_temp = pool.map(prepare_data, indices)\n",
    "\n",
    "# Flatten the list of lists\n",
    "temp = [datum for sublist in parallel_temp for datum in sublist]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ad0c066e-6e0f-49e1-8854-d005c1bc4154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time per datum:6.083787679672241e-05sec\n",
      "Datums per Second::16437.12852342464 - WriteBatch\n",
      "time to WRITE 60000 data::3.6502726078033447sec\n"
     ]
    }
   ],
   "source": [
    "tic = time()\n",
    "\n",
    "# Use multiprocessing to prepare data in parallel\n",
    "with Pool(num_processes) as pool:\n",
    "    parallel_temp = pool.map(prepare_data, indices)\n",
    "\n",
    "# Flatten the list of lists\n",
    "temp = [datum for sublist in parallel_temp for datum in sublist]\n",
    "n=100 # Write 100 datums within batch\n",
    "databatches = [temp[i:i+n] for i in range(0,len(temp),n)]\n",
    "\n",
    "\n",
    "for datum in databatches:\n",
    "    myDBWriter.write_batch(datum)\n",
    "Dt = time() - tic\n",
    "NData = len(mnist_dataset)\n",
    "print(\"Time per datum:{}sec\".format(Dt/NData))\n",
    "print(\"Datums per Second::{} - WriteBatch\".format(NData/Dt))\n",
    "print(\"time to WRITE {} data::{}sec\".format(NData,Dt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3579981c-051c-4370-b84a-0459e6f84b89",
   "metadata": {},
   "source": [
    "## Therefore we gained further x2 speed up, overall x6 in comparison with standard HDF5 for writing the data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f17faf9-8f63-4878-8206-1d4bfc75b318",
   "metadata": {},
   "source": [
    "# Reading Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62da0581-05cc-4f26-8a81-efd5a7b458cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "class HDF5Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self,filepath, transform=None):\n",
    "        self.filepath = filepath\n",
    "        self.transform = transform\n",
    "        self.hdf5 = None\n",
    "        self.length=60000\n",
    "        \n",
    "    def open_hdf5(self):\n",
    "\n",
    "        self.hdf5 = h5py.File(self.filepath,'r')\n",
    "        self.length = self.hdf5['inputs'].shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        if self.hdf5 is None:\n",
    "            self.open_hdf5()\n",
    "        img = self.hdf5['inputs'][idx][:] # Do loading here\n",
    "        labels = self.hdf5['labels'][idx]\n",
    "        \n",
    "        return img.astype(np.float32), labels.astype(np.float32)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a3a1e21-c7d0-4886-ac26-72f9cadcb4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers=16\n",
    "trainset_hdf5 = HDF5Dataset(filepath='mnist.hdf5')\n",
    "batch_size=128\n",
    "dataloader_hdf5 = torch.utils.data.DataLoader(trainset_hdf5, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=num_workers,drop_last=True,pin_memory=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "96d5db68-d41f-43d2-8ceb-784266a97ba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time per batch:0.0028857471596481455sec\n",
      "Batches per Second::346.5307058023505 - SERIAL\n",
      "time for 1 epoch, BatchSteps468, Dt::1.350529670715332sec\n"
     ]
    }
   ],
   "source": [
    "tic = time()\n",
    "#for data in progressbar(dataloader_hdf5):\n",
    "for data in dataloader_hdf5:\n",
    "    img,label=data\n",
    "Dt = time() - tic\n",
    "NSteps = len(dataloader_hdf5)\n",
    "print(\"Time per batch:{}sec\".format(Dt/NSteps))\n",
    "print(\"Batches per Second::{} - SERIAL\".format(NSteps/Dt))\n",
    "print(\"time for 1 epoch, BatchSteps{}, Dt::{}sec\".format(NSteps,Dt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ebb0538-d4a5-4426-a051-b365ccb7e75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rocksdb\n",
    "import pickle\n",
    "\n",
    "\n",
    "\n",
    "# See https://github.com/pytorch/vision/issues/689\n",
    "class RocksDBDatasetDemo(torch.utils.data.Dataset):\n",
    "    # This Dataset class Reads inputs, labels that are in numpy format \n",
    "    # The LMDB Dataset class MUST include meta information (provided under the key 'meta') that include information \n",
    "    # of inputs, labels shape and data type. \n",
    "\n",
    "    # TODO: I don't need map_size when reading the database \n",
    "    def __init__(self, filepath,  transform=None, num_workers=6):\n",
    "        super().__init__()\n",
    "        self.flname_db = filepath\n",
    "\n",
    "        self.db = None\n",
    "        self.transform = transform\n",
    "        self.num_workers = num_workers\n",
    "        \n",
    "        self._open_rocks()\n",
    "        \n",
    "    def _open_rocks(self):\n",
    "\n",
    "\n",
    "        # ==========================================================================\n",
    "        # TODO : they need further investigation \n",
    "        # Some good behaving defaults \n",
    "        opts = rocksdb.Options()\n",
    "        opts.create_if_missing = True\n",
    "        opts.max_open_files = 300000\n",
    "        opts.write_buffer_size = 67108864\n",
    "        opts.max_write_buffer_number = 30 # 3 default\n",
    "        opts.target_file_size_base = 67108864  # default 67108864, value starting 7: 7340032 input.nbytes\n",
    "        opts.paranoid_checks=False\n",
    "\n",
    "        \n",
    "        opts.table_factory = rocksdb.BlockBasedTableFactory(\n",
    "                format_version=5,\n",
    "                filter_policy=rocksdb.BloomFilterPolicy(10), # 10 default\n",
    "               #block_size=4*1024, # 16KB\n",
    "                block_cache=rocksdb.LRUCache( 32 * (1024 ** 3)), # 2 * (1024 ** 3) default\n",
    "                block_cache_compressed=rocksdb.LRUCache(16 * (1024 ** 2)))\n",
    "        # ==================================================================================\n",
    "\n",
    "        opts.IncreaseParallelism(self.num_workers)\n",
    "        \n",
    "        self.db = rocksdb.DB(self.flname_db, opts,read_only=True)\n",
    "        \n",
    "        it = self.db.iterkeys()\n",
    "        it.seek_to_first()\n",
    "        self.keys = list(it)\n",
    "                \n",
    "        meta_key  = list(filter(lambda x : 'meta'.encode('ascii') in x, self.keys ))[0]\n",
    "        meta = self.db.get(meta_key)\n",
    "        meta = pickle.loads(meta)\n",
    "\n",
    "        self.inputs_shape = meta['inputs_shape']\n",
    "        self.inputs_dtype = meta['inputs_dtype']\n",
    "\n",
    "        self.labels_shape = meta['labels_shape']\n",
    "        self.labels_dtype = meta['labels_dtype']\n",
    "\n",
    "        # \n",
    "        self.inputs_keys = list(filter(lambda x : 'inputs'.encode('ascii') in x, self.keys ))\n",
    "        self.labels_keys = list(filter(lambda x : 'labels'.encode('ascii') in x, self.keys ))\n",
    "\n",
    "        self.length = len(self.inputs_keys) # Alternative definition \n",
    "\n",
    "    def get_inputs_labels(self,idx):\n",
    "        \n",
    "        key_input = self.inputs_keys[idx]\n",
    "        key_label = self.labels_keys[idx]\n",
    "        \n",
    "        \n",
    "        inputs = self.db.get(key_input)\n",
    "        inputs = np.frombuffer(inputs, dtype= self.inputs_dtype).reshape(*self.inputs_shape)\n",
    "\n",
    "        labels = self.db.get(key_label)\n",
    "        labels = np.frombuffer(labels, dtype= self.labels_dtype) # .reshape(*self.labels_shape)\n",
    "\n",
    "        return inputs, labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #if self.db is None:\n",
    "        #    self._open_rocks()\n",
    "\n",
    "        inputs, labels = self.get_inputs_labels(idx)\n",
    "\n",
    "        #if self.transform is not None:\n",
    "        #    inputs,labels = self.transform(inputs,labels)\n",
    "\n",
    "        return inputs.astype(np.float32), labels.astype(np.float32)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "46035191-077c-41c3-bddd-9627a2488f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers=16\n",
    "trainset_rocks = RocksDBDatasetDemo('mnist_rocks_batch.db/',num_workers=num_workers)\n",
    "batch_size= 128\n",
    "dataloader_rocks = torch.utils.data.DataLoader(trainset_rocks, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=num_workers,drop_last=True,pin_memory=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7e904196-d750-4d48-862e-144b297e52fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time per batch:0.0012967566139677651sec\n",
      "Batches per Second::771.1547326835984 - SERIAL\n",
      "time for 1 epoch, BatchSteps468, Dt::0.6068820953369141sec\n"
     ]
    }
   ],
   "source": [
    "tic = time()\n",
    "for data in dataloader_rocks:\n",
    "    img,label=data\n",
    "Dt = time() - tic\n",
    "NSteps = len(dataloader_rocks)\n",
    "print(\"Time per batch:{}sec\".format(Dt/NSteps))\n",
    "print(\"Batches per Second::{} - SERIAL\".format(NSteps/Dt))\n",
    "print(\"time for 1 epoch, BatchSteps{}, Dt::{}sec\".format(NSteps,Dt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b51fce-9d91-4aae-81d5-87d83028dcfb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7045cc-b2df-4b1a-971e-b6de5c2b5828",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef464160-cee1-4370-8ce5-3d91e721e7ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb4e7e0-e27b-4bbc-81f8-fc90c11fe5e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5ef7e7-3bf2-4971-83b3-1363c22ac018",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "abb76551-a735-47ad-bf6d-1c90f1fb96e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(r'../../')\n",
    "from ssg2.data.rocksdbutils import RocksDBDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "49c57d86-8582-4261-862a-a0cc4cde6615",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "num_workers=16\n",
    "trainset_rocks = RocksDBDataset('mnist_RDB_v2.db',num_workers=num_workers)\n",
    "batch_size= 128\n",
    "dataloader_rocks = torch.utils.data.DataLoader(trainset_rocks, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=num_workers,drop_last=True,pin_memory=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "32dfc032-719c-4b99-8051-cb1604e403ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Note, the first 2 epochs are slower due to automatic caching of the data, that happens progressively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "70d138b4-a55c-463f-a3a2-cf3c5a97bb21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time per batch:0.0012125912894550553sec\n",
      "Batches per Second::824.6801776461756 - SERIAL\n",
      "time for 1 epoch, BatchSteps468, Dt::0.5674927234649658sec\n",
      "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n"
     ]
    }
   ],
   "source": [
    "tic = time()\n",
    "for data in dataloader_rocks:\n",
    "    img,label=data\n",
    "Dt = time() - tic\n",
    "NSteps = len(dataloader_rocks)\n",
    "print(\"Time per batch:{}sec\".format(Dt/NSteps))\n",
    "print(\"Batches per Second::{} - SERIAL\".format(NSteps/Dt))\n",
    "print(\"time for 1 epoch, BatchSteps{}, Dt::{}sec\".format(NSteps,Dt))\n",
    "print(\"XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ed1ba7-c598-44fe-93dc-87eb76f82bdd",
   "metadata": {},
   "source": [
    "## Speedup over HDF5 ~ 1.35/0.56 = 2.4 times "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5700603e-ae14-4449-b502-7460ff1053ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.4107142857142856"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1.35/0.56"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e10b87-b077-4ada-ad72-bc273976c64d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
